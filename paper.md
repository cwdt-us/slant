
---
title: 'slant: A Python package for identifying and quantifying bias in texts'
tags:
  - Python
  - experiments
  - vignette experiments
  - placebo conditions
authors:
  - name: Charles Crabtree^[co-first author] # note this makes a footnote saying 'co-first author'
    orcid: 0000-0001-5144-8671
    affiliation: 1 # (Multiple affiliations must be quoted)
  - name: William M. Marx^[co-first author] # note this makes a footnote saying 'co-first author'
    orcid: 0000-0002-0408-0517
    affiliation: 1
#    orcid: 
affiliations:
 - name: Department of Government, Dartmouth College
   index: 1
date: 18 August 2021
bibliography: paper.bib
---

# Summary

Medical and pharmacological researchers have long relied on placebo-controlled studies to isolate
and identify the causal effect of a treatment from the causal effect of being treated. In recent
years, social scientists have increasingly turned to this study design in survey experiments,
particularly vignette-based experiments. In these experiments, researchers present survey
respondents with a variety of texts, randomizing specific attributes. For example, a researcher who
wants to study the effect of negative campaigning on public support for political candidates might
create two vignettes. One could describe a candidate's policy platform, while the other could
include that text but also some criticism of the other candidate as well. The idea here would be
that researchers would randomly assign these texts to respondents and see if support for the
candidate varies across conditions. While placebo conditions are now widely used and vitally
important to the inferences researchers draw from survey experiments, there’s been little
methodological work on either when they should be used or how to create them. `@portervelez:2021`
introduce a new approach to creating and selecting placebo conditions - agnostic generation via
OpenAI’s GPT-2, a widely used language model. `slant` enables researchers to both reliably and
efficiently assess the degree to which texts generated by this, other tools, or through manual
production contain bias. The governing concern here is that researchers not produce texts for
research that contain biases against individuals portrayed in them according to race/ethnicity,
gender, sexual and religious orientation, or political affiliation and ideology.

# Statement of need

Our tool, `slant`, is a Python package for identifying and quantifying bias in texts. It provides a
simple interface to our implementation of VADER, which reproduces results of @Hutto\_Gilbert\_2014
VADER tool up to 70x faster. To identify bias in substantive content, `slant` allows users to
quickly check if any biasing words exist in a text using a modified @Aho1975EfficientSM string
search. While `slant` includes a default list of potentially biasing words, adapted from Wikipedia’s
list of ethnic slurs, it is recommended that researchers use their own project-specific lists to
best accomplish this task. Finally, `slant` allows researchers to quickly view the N most frequent
non-stop words in a given text or texts.

`slant` was designed to be used by researchers who create and implement text-based survey
experiments, particularly those that rely on vignettes. We expect that it will be particularly
useful to researchers doing work in substantive areas where the bias contained in manually or
automatically generated texts might be most problematic, such as in race and ethnic politics.

# Acknowledgements

We acknowledge advice from Matt Golder and Sona Golder during the genesis of this project.

# References
